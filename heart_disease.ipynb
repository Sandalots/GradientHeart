{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e5d341",
   "metadata": {},
   "source": [
    "## **Gradient Boosting in Regards to Heart Disease Classification, plus SHAP Visualisation for explaining the Models Nature**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e51fe",
   "metadata": {},
   "source": [
    "Required external pip imports will be retrieved and downloaded in the below pip install code cell for the notebook requirments to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas pd numpy scikit-learn matplotlib seaborn kagglehub shap xgboost lightgbm dask dask-ml distributed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2120fc",
   "metadata": {},
   "source": [
    "Now the notebook will import the downloaded pip modules ensuring they can be linked, retrieved and used globally in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for performing more advanced operations on arrays, such as converting the heart disease dataset columns to float64 and int64 respectively as well as creating partions from the x training set\n",
    "import pandas as pd # for converting the heart disease dataset to a more robustly interfacable pandas dataframe array for working with the various ML Models and helper methods\n",
    "import matplotlib.pyplot as plt # plotting roc scores et al of the gradient boosting models\n",
    "import seaborn as sns # for heatmap plotting the correlation matrix of the models\n",
    "\n",
    "import kagglehub # for retrieving and downloading the heart disease dataset from kaggle\n",
    "\n",
    "# Gradient Boosting Models\n",
    "import xgboost as xgb # the xgboost model\n",
    "import lightgbm as lgb # the lightgbm model\n",
    "import shap # explaining how the two gradient boosting models made their predictions, i.e. feature importance\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler # for scaling the heart disease dataset features\n",
    "from sklearn.model_selection import train_test_split # for splitting the heart disease dataset into training, testing and validation sets\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score # for generating model classification reports, confusion matrices and roc auc scores\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV # for selecting best hyperparameters for the gradient boosting models, both using 10 fold cross validation\n",
    "\n",
    "# Dask for distributed learning of the LightGBM model\n",
    "from dask.distributed import Client # contructing the dask client for distributed learning\n",
    "from dask_ml.model_selection import GridSearchCV as DaskGridSearchCV # selecting best hyperparameters for the LightGBM model using dask distributed learning\n",
    "import dask.array as da # data collection for the dask distributed learning client\n",
    "\n",
    "import warnings # hiding warnings to clean up evaluation output\n",
    "\n",
    "warnings.filterwarnings('ignore') # filter out any warnings in the notebook cell outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab89723",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"redwankarimsony/heart-disease-data\") # get path to downloaded kaggle heart disease dataset files\n",
    "\n",
    "print(\"Path to dataset files:\", path) # output to user that path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = path + \"/heart_disease_uci.csv\" # retrieve the dataset by appending the filename to the enclosing path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aa4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df = pd.read_csv(heart_data) #convert heart disease csv data to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befaacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df.head() # head the dataset to see that it has been converted to a pandas dataframe successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e47768",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df.shape # get rows and columns size of the heart disease dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42de81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df[\"num\"] = heart_disease_df[\"num\"].apply(lambda x: 1 if x > 0 else 0) # make target num feature binary, either true 1 or 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ca052",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df.head() # show newly binarised target feature has been applied successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df.info() # output number of columns, along with their names, data type and how many non-null values they contain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7634c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df = heart_disease_df.drop(columns=[\"id\"]) # drop index id column as it does not need to be represented \n",
    "\n",
    "heart_disease_df.head() # show dataset with the dropped id column, notying we still have an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = heart_disease_df.select_dtypes(include=['object']).columns.tolist() # find categorical columns in the dataset by filtering for object data types\n",
    "\n",
    "categorical_cols # output textually the categorical columns found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642bdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df = pd.get_dummies(heart_disease_df, columns=categorical_cols, drop_first=True) # one hot encode the categorical columns found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df.isnull().sum() # search for missing values in the heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a08149",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.5 # if over half the values in a column are missing, drop the column\n",
    "\n",
    "for col in heart_disease_df.columns: # iterate through the heart disease  columns\n",
    "    missing_fraction = heart_disease_df[col].isnull().mean() # compute the fraction of missing values in the column\n",
    "\n",
    "    if missing_fraction > missing_threshold: # if the missing fraction is greater than the threshold\n",
    "        heart_disease_df.drop(columns=[col], inplace=True) # drop the column\n",
    "\n",
    "    else: # if the missing fraction is less than the threshold\n",
    "        if heart_disease_df[col].dtype in [np.float64, np.int64]: # and if the column is numerical\n",
    "            heart_disease_df[col] = heart_disease_df[col].fillna(heart_disease_df[col].median()) # fill missing values with the median value of the column\n",
    "\n",
    "        else: # if the column is categorical\n",
    "            heart_disease_df[col] = heart_disease_df[col].fillna(heart_disease_df[col].mode()[0]) # fill missing values with the mode value of the column\n",
    "\n",
    "heart_disease_df.isnull().sum() # now verify that there are no missing values in the heart disease dataset\n",
    "\n",
    "heart_disease_df.head() # output the cleaned heart disease dataset with no remaining missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dffd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = heart_disease_df.select_dtypes(include=[np.float64, np.int64]).columns.tolist() # get numerical columns by filtering for float64 and int64 data types\n",
    "\n",
    "numerical_cols.remove(\"num\") # remove the target variable column from the list of numerical columns\n",
    "\n",
    "scaler = StandardScaler() # apply standard scaling to the numerical columns to bring them to a common scale\n",
    "\n",
    "heart_disease_df[numerical_cols] = scaler.fit_transform(heart_disease_df[numerical_cols]) # fit and transform the numerical columns using the standard scaler\n",
    "\n",
    "heart_disease_df.head() # output the heart disease dataset with scaled numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10)) # setup a plot\n",
    "\n",
    "correlation_matrix = heart_disease_df.corr() # get correlation matrix of the heart disease dataset\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True) # heatmap plot the correlation matrix\n",
    "\n",
    "plt.title('Correlation Heatmap of Heart Disease Features', fontsize=16) \n",
    "\n",
    "plt.show() # output heatmap plot of the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease_df.drop(\"num\", axis=1) # get the features of heart disease by dropping the target variable column\n",
    "\n",
    "y = heart_disease_df[\"num\"] # get the target variable column of heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65) # split the heart disease dataset into training and testing sets with an 80-20 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678c37c",
   "metadata": {},
   "source": [
    "### **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626aee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(eval_metric='logloss', random_state=65) # setup the xgboost classifier model\n",
    "\n",
    "xgb_clf.fit(X_train, y_train) # train the xgboost classifier model on the training data of the heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de90cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_xgb = xgb_clf.predict(X_test) # make predictions\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_xgb)) # output confusion matrix of the xgboost model\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb)) # output classification report of the xgboost model\n",
    "\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_pred_xgb)) # output ROC AUC score of the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameter for grid search\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1cc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_grid = GridSearchCV(xgb.XGBClassifier(random_state=65),\n",
    "                      xgb_param_grid,\n",
    "                      cv=10,\n",
    "                      scoring='roc_auc',\n",
    "                      n_jobs=-1,\n",
    "                      verbose=1) # setup the xgboost grid search with 10 fold cross validation to find best hyperparameters from the parameter grid above\n",
    "\n",
    "xgb_grid.fit(X_train, y_train) # perform the grid search on the training data\n",
    "\n",
    "print('Best XGBoost Params:', xgb_grid.best_params_) # report back the best hyperparameters found from the grid search\n",
    "print('Best XGBoost CV ROC AUC:', xgb_grid.best_score_) # report back the best cross-validated ROC AUC score from the grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe48f7e",
   "metadata": {},
   "source": [
    "## **LightGBM Non-Distrubuted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaafddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(random_state=65) # setup the lightgbm classifier model\n",
    "\n",
    "lgb_clf.fit(X_train, y_train) # train the lightgbm classifier model on the training data of the heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f257841",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgb = lgb_clf.predict(X_test) # make predictions\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_lgb)) # output confusion matrix of the lightgbm model all using the testing data split\n",
    "\n",
    "print(classification_report(y_test, y_pred_lgb)) # output the classification report of the lightgbm model\n",
    "\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_pred_lgb)) # output ROC AUC score of the lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM hyperparameter grid search\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'min_child_samples': [5, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d40b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_grid = RandomizedSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=65),\n",
    "    lgb_param_grid,\n",
    "    n_iter=20,\n",
    "    cv=10,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    random_state=65\n",
    ") # use randomized search cv to find best hyperparameters for lightgbm model for 10 fold cross validation\n",
    "\n",
    "lgb_grid.fit(X_train, y_train) # perform the randomized search on the training data\n",
    "\n",
    "print('Best LightGBM Params:', lgb_grid.best_params_)  # output the best hyperparameters found for lightgbm model\n",
    "print('Best LightGBM CV ROC AUC:', lgb_grid.best_score_) # output the best cross-validated ROC AUC score from the randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024c3bd",
   "metadata": {},
   "source": [
    "## **LightGBM Distrubuted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96887251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with distributed learning using Dask\n",
    "\n",
    "# setup a dask client for applying distributed learning to the lightgbm model, with 2 workers, hyperthreading and 2GB memory limit per worker\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='2GB')\n",
    "\n",
    "print(client) # output parameters of the dask client\n",
    "\n",
    "print(f\"Dashboard link: {client.dashboard_link}\") # output the dashboard link for the dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e580acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training and testing data to dask arrays with appropriate chunk sizes for distributed learning using dask\n",
    "X_train_dask = da.from_array(X_train.values, chunks=(len(X_train)//2, X_train.shape[1])) # convert X training set to dask array with 2 partitions\n",
    "y_train_dask = da.from_array(y_train.values, chunks=len(y_train)//2) # convert y training set to dask array with 2 partitions\n",
    "X_test_dask = da.from_array(X_test.values, chunks=(len(X_test)//2, X_test.shape[1])) # convert X testing set to dask array with 2 partitions\n",
    "\n",
    "print(f\"Training data partitions: {X_train_dask.npartitions}\") # output number of partitions in the dask array\n",
    "print(f\"Training data shape: {X_train_dask.shape}\") # output shape of the dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM with distributed learning\n",
    "lgb_dist_clf = lgb.LGBMClassifier(\n",
    "    random_state=65,\n",
    "    n_jobs=-1  # utilize all available CPU cores\n",
    ")\n",
    "\n",
    "lgb_dist_clf.fit(X_train, y_train) # fit on the heart disease training data\n",
    "\n",
    "y_pred_lgb_dist = lgb_dist_clf.predict(X_test) # make predictions using distrubted LightGBM model\n",
    "\n",
    "print(\"LightGBM Distributed Learning Results:\") # header for results\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_lgb_dist))  # output confusion matrix of the distributed lightgbm model\n",
    "\n",
    "print(classification_report(y_test, y_pred_lgb_dist)) # output classification report of the distributed lightgbm model\n",
    "\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_pred_lgb_dist)) # output ROC AUC score of the distributed lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_dist_grid = DaskGridSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=65, n_jobs=1),\n",
    "    lgb_param_grid,\n",
    "    cv=10,\n",
    "    scoring='roc_auc'\n",
    ") # setup dask grid search cv for distributed learning to find best hyperparameters for lightgbm model\n",
    "\n",
    "lgb_dist_grid.fit(X_train, y_train) # perform the grid search on the training data\n",
    "\n",
    "print('Best LightGBM Distributed Params:', lgb_dist_grid.best_params_) # output the best hyperparameters found for distributed lightgbm model\n",
    "\n",
    "print('Best LightGBM Distributed CV ROC AUC:', lgb_dist_grid.best_score_) # output the best cross-validated ROC AUC score from the distributed grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close() # close the dask client as we have found best hyperparameters and distributed learning is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ffb45",
   "metadata": {},
   "source": [
    "### **SHAP**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation for XGBoost model\n",
    "\n",
    "explainer_xgb = shap.TreeExplainer(xgb_clf) # setup SHAP tree explainer for the xgboost model\n",
    "shap_values_xgb = explainer_xgb.shap_values(X_test.values) # compute SHAP values for the xgboost model using the testing data of the heart disease dataset\n",
    "\n",
    "shap.summary_plot(shap_values_xgb, X_test, show=False) # summary plot the calculated SHAP values for the xgboost model\n",
    "\n",
    "plt.title('SHAP Summary Plot for XGBoost')\n",
    "\n",
    "plt.show() # output SHAP summary plot for the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation for non-distributed LightGBM\n",
    "    \n",
    "explainer_lgb = shap.TreeExplainer(lgb_clf) # setup SHAP tree explainer for the non-distributed lightgbm model\n",
    "shap_values_lgb = explainer_lgb.shap_values(X_test.values) # compute SHAP values for the non-distributed lightgbm model using the testing data of the heart disease dataset\n",
    "\n",
    "shap.summary_plot(shap_values_lgb, X_test, show=False) # summary plot the calculated SHAP values for the non-distributed lightgbm model\n",
    "\n",
    "plt.title('SHAP Summary Plot for LightGBM')\n",
    "\n",
    "plt.show() # output SHAP summary plot for the non-distributed lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation for LightGBM with Dask Distributed Learning\n",
    "    \n",
    "explainer_lgb_dist = shap.TreeExplainer(lgb_dist_clf) # setup SHAP tree explainer for the lightgbm model trained with dask distributed learning\n",
    "shap_values_lgb_dist = explainer_lgb_dist.shap_values(X_test.values) # compute SHAP values for the lightgbm model trained with dask distributed learning using the testing data of the heart disease dataset\n",
    "\n",
    "shap.summary_plot(shap_values_lgb_dist, X_test, show=False) # plot the calculated SHAP values for the lightgbm model trained with dask distributed learning\n",
    "\n",
    "plt.title('SHAP Summary Plot for LightGBM with Dask')\n",
    "\n",
    "plt.show() # output SHAP summary plot for the lightgbm model trained with dask distributed learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
