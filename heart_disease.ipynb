{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12e5d341",
   "metadata": {},
   "source": [
    "## **Gradient Boosting in Regards to Heart Disease Classification, plus SHAP Visualisation for explaining the Models Nature**\n",
    "*Group 10* <br>\n",
    "*08/12/2025*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62941a6b",
   "metadata": {},
   "source": [
    "### **Data Preparation and Import**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056e51fe",
   "metadata": {},
   "source": [
    "Required external pip imports will be retrieved and downloaded in the below pip install code cell for the notebook requirments to be resolved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c0668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install pandas pd numpy scikit-learn matplotlib seaborn kagglehub shap xgboost lightgbm dask dask-ml distributed ipywidgets "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2120fc",
   "metadata": {},
   "source": [
    "Now the notebook will import the downloaded pip modules ensuring they can be linked, retrieved and used globally in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2bb822",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # for performing more advanced operations on arrays, such as converting the heart disease dataset columns to float64 and int64 respectively as well as creating partions from the x training set\n",
    "import pandas as pd # for converting the heart disease dataset to a more robustly interfacable pandas dataframe array for working with the various ML Models and helper methods\n",
    "import matplotlib.pyplot as plt # plotting roc scores et al of the gradient boosting models\n",
    "import seaborn as sns # for heatmap plotting the correlation matrix of the models\n",
    "\n",
    "import kagglehub # for retrieving and downloading the heart disease dataset from kaggle\n",
    "\n",
    "# Gradient Boosting Models\n",
    "import xgboost as xgb # the xgboost model\n",
    "import lightgbm as lgb # the lightgbm model\n",
    "\n",
    "import time # for measuring inference time of the models\n",
    "from math import pi # for computing pie angles in evaluation plots\n",
    "\n",
    "# SHAP \n",
    "import shap # explaining how the two gradient boosting models made their predictions, i.e. feature importance in regards to the two grad boost models created deeper within this notebook\n",
    "\n",
    "# Scikit Learn Methods\n",
    "from sklearn.preprocessing import StandardScaler # for scaling the heart disease dataset features\n",
    "from sklearn.model_selection import train_test_split # for splitting the heart disease dataset into training, testing and validation sets\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score, f1_score  # for getting the scores, precision, recall, accuracy and f1 scores for evaulating the different metric scores for each model\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score # for generating model classification reports, confusion matrices and roc auc scores\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV # for selecting best hyperparameters for the gradient boosting models, both using 10 fold cross validation\n",
    "\n",
    "# Dask for distributed learning of the LightGBM model\n",
    "from dask.distributed import Client # contructing the dask client for distributed learning\n",
    "from dask_ml.model_selection import GridSearchCV as DaskGridSearchCV # selecting best hyperparameters for the LightGBM model using dask distributed learning\n",
    "import dask.array as da # data collection for the dask distributed learning client\n",
    "\n",
    "import warnings # hiding warnings to clean up evaluation output\n",
    "\n",
    "warnings.filterwarnings('ignore') # filter out any warnings in the notebook cell outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836bd4ae",
   "metadata": {},
   "source": [
    "### **Data Exploration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ab89723",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = kagglehub.dataset_download(\"redwankarimsony/heart-disease-data\") # get path to downloaded kaggle heart disease dataset files\n",
    "\n",
    "print(\"Path to kaggle files:\", path) # output to user that path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200fcf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_data = path + \"/heart_disease_uci.csv\" # retrieve the dataset by appending the filename to the enclosing path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39aa4a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df = pd.read_csv(heart_data) #convert heart disease csv data to a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befaacbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heart Disease Dataset before Preprocessing:\")\n",
    "heart_disease_df.head() # head the dataset to see that it has been converted to a pandas dataframe successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e47768",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df.shape # get rows and columns size of the heart disease dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42de81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarisation of the Target 'Num' Feature\n",
    "heart_disease_df[\"num\"] = heart_disease_df[\"num\"].apply(lambda x: 1 if x > 0 else 0) # make target num feature binary, either true 1 or 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09ca052",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heart Disease Dataset after Target Binarisation:\")\n",
    "heart_disease_df.head() # show newly binarised target feature has been applied successfully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df86c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df.info() # output number of columns, along with their names, data type and how many non-null values they contain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3094e43e",
   "metadata": {},
   "source": [
    "### **Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7634c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df = heart_disease_df.drop(columns=[\"id\"]) # drop index id column as it does not need to be represented \n",
    "\n",
    "print(\"Heart Disease Dataset without ID Column:\")\n",
    "heart_disease_df.head() # show dataset with the dropped id column, notying we still have an index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2fa92c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = heart_disease_df.select_dtypes(include=['object']).columns.tolist() # find categorical columns in the dataset by filtering for object data types\n",
    "\n",
    "print(\"Categorical Columns in Heart Disease Dataset:\")\n",
    "categorical_cols # output textually the categorical columns found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642bdc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease_df = pd.get_dummies(heart_disease_df, columns=categorical_cols, drop_first=True) # one hot encode the categorical columns found above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd6b226",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing Values in Each Feature:\")\n",
    "heart_disease_df.isnull().sum() # search for missing values in the heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a08149",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 0.5 # if over half the values in a column are missing, drop the column\n",
    "\n",
    "for col in heart_disease_df.columns: # iterate through the heart disease  columns\n",
    "    missing_fraction = heart_disease_df[col].isnull().mean() # compute the fraction of missing values in the column\n",
    "\n",
    "    if missing_fraction > missing_threshold: # if the missing fraction is greater than the threshold\n",
    "        heart_disease_df.drop(columns=[col], inplace=True) # drop the column\n",
    "\n",
    "    else: # if the missing fraction is less than the threshold\n",
    "        if heart_disease_df[col].dtype in [np.float64, np.int64]: # and if the column is numerical\n",
    "            heart_disease_df[col] = heart_disease_df[col].fillna(heart_disease_df[col].median()) # fill missing values with the median value of the column\n",
    "\n",
    "        else: # if the column is categorical\n",
    "            heart_disease_df[col] = heart_disease_df[col].fillna(heart_disease_df[col].mode()[0]) # fill missing values with the mode value of the column\n",
    "\n",
    "heart_disease_df.isnull().sum() # now verify that there are no missing values in the heart disease dataset\n",
    "\n",
    "heart_disease_df.head() # output the cleaned heart disease dataset with no remaining missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16dffd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = heart_disease_df.select_dtypes(include=[np.float64, np.int64]).columns.tolist() # get numerical columns by filtering for float64 and int64 data types\n",
    "\n",
    "numerical_cols.remove(\"num\") # remove the target variable column from the list of numerical columns\n",
    "\n",
    "scaler = StandardScaler() # apply standard scaling to the numerical columns to bring them to a common scale\n",
    "\n",
    "heart_disease_df[numerical_cols] = scaler.fit_transform(heart_disease_df[numerical_cols]) # fit and transform the numerical columns using the standard scaler\n",
    "\n",
    "heart_disease_df.head() # output the heart disease dataset with scaled numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a4a9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 10)) # setup a plot\n",
    "\n",
    "correlation_matrix = heart_disease_df.corr() # get correlation matrix of the heart disease dataset\n",
    "\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True) # heatmap plot the correlation matrix\n",
    "\n",
    "plt.title('Correlation Heatmap of Heart Disease Features', fontsize=16) \n",
    "\n",
    "plt.show() # output heatmap plot of the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138e5ebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = heart_disease_df.drop(\"num\", axis=1) # get the features of heart disease by dropping the target variable column\n",
    "\n",
    "y = heart_disease_df[\"num\"] # get the target variable column of heart disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1066b7ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=65) # split the heart disease dataset into training and testing sets with an 80-20 split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d18fa8aa",
   "metadata": {},
   "source": [
    "## **Introducing the Gradient Boosting Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a678c37c",
   "metadata": {},
   "source": [
    "### **Training the XGBoost Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626aee83",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(eval_metric='logloss', random_state=65) # setup the xgboost classifier model\n",
    "\n",
    "xgb_clf.fit(X_train, y_train) # train the xgboost classifier model on the training data of the heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de90cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "y_pred_xgb = xgb_clf.predict(X_test) # make predictions\n",
    "inference_time_xgb = time.time() - start_time\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_xgb)) # output confusion matrix of the xgboost model\n",
    "\n",
    "print(classification_report(y_test, y_pred_xgb)) # output classification report of the xgboost model\n",
    "\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_pred_xgb)) # output ROC AUC score of the xgboost model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2037a6",
   "metadata": {},
   "source": [
    "### **Finding Best Hyperparameters for XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974f010",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost hyperparameters for grid search\n",
    "xgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'min_child_weight': [1, 3, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1cc6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting up xgboost to search the grid in the above cell to find best hyperparameters\n",
    "xgb_grid = GridSearchCV(xgb.XGBClassifier(random_state=65),\n",
    "                      xgb_param_grid, # use the custom param grid\n",
    "                      cv=10, # 10 cross folds validations\n",
    "                      scoring='roc_auc', # score based on ROC AUC\n",
    "                      n_jobs=-1, # use every cpu core available\n",
    "                      verbose=1) # setup the xgboost grid search with 10 fold cross validation to find best hyperparameters from the parameter grid above\n",
    "\n",
    "xgb_grid.fit(X_train, y_train) # perform the grid search on the training data\n",
    "\n",
    "print('Best XGBoost Params:', xgb_grid.best_params_) # report back the best hyperparameters found from the grid search\n",
    "print('Best XGBoost CV ROC AUC:', xgb_grid.best_score_) # report back the best cross-validated ROC AUC score from the grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe48f7e",
   "metadata": {},
   "source": [
    "## **Training a Non-Distributed LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaafddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(random_state=65) # setup the lightgbm classifier model\n",
    "\n",
    "lgb_clf.fit(X_train, y_train) # train the lightgbm classifier model on the training data of the heart disease dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f257841",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lgb = lgb_clf.predict(X_test) # make predictions\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_lgb)) # output confusion matrix of the lightgbm model all using the testing data split\n",
    "\n",
    "print(classification_report(y_test, y_pred_lgb)) # output the classification report of the lightgbm model\n",
    "\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_pred_lgb)) # output ROC AUC score of the lightgbm model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c606714b",
   "metadata": {},
   "source": [
    "### **Finding Best Hyperparameters for the Non-Distributed LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db2694e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM hyperparameter grid for the randomised lgb search using cross validation\n",
    "lgb_param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_depth': [-1, 5, 10],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'subsample': [0.8, 0.9, 1.0],\n",
    "    'min_child_samples': [5, 10, 20]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d40b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup the randomised search from the hyperparameters defined in the grid above\n",
    "lgb_grid = RandomizedSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=65), # instance the lgbm classifier\n",
    "    lgb_param_grid, # use the cusotm hyperparameter grid\n",
    "    n_iter=20, # 20 random search iterations\n",
    "    cv=10, # use 10 fold cross validation\n",
    "    scoring='roc_auc', # score based on ROC AUC\n",
    "    n_jobs=-1, # use every cpu core available\n",
    "    verbose=1, # output more detailed lgb output\n",
    "    random_state=65\n",
    ") # use randomized search cv to find best hyperparameters for lightgbm model for 10 fold cross validation\n",
    "\n",
    "lgb_grid.fit(X_train, y_train) # perform the randomized search on the training data\n",
    "\n",
    "print('Best LightGBM Params:', lgb_grid.best_params_)  # output the best hyperparameters found for lightgbm model\n",
    "print('Best LightGBM CV ROC AUC:', lgb_grid.best_score_) # output the best cross-validated ROC AUC score from the randomized search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2024c3bd",
   "metadata": {},
   "source": [
    "## **Training a Distributed LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96887251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM with distributed learning using Dask\n",
    "\n",
    "# setup a dask client for applying distributed learning to the lightgbm model, with 2 workers, hyperthreading and 2GB memory limit per worker\n",
    "client = Client(n_workers=2, threads_per_worker=2, memory_limit='2GB')\n",
    "\n",
    "print(client) # output parameters of the dask client\n",
    "\n",
    "print(f\"Dashboard link: {client.dashboard_link}\") # output the dashboard link for the dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e580acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert training and testing data to dask arrays with appropriate chunk sizes for distributed learning using dask\n",
    "X_train_dask = da.from_array(X_train.values, chunks=(len(X_train)//2, X_train.shape[1])) # convert X training set to dask array with 2 partitions\n",
    "y_train_dask = da.from_array(y_train.values, chunks=len(y_train)//2) # convert y training set to dask array with 2 partitions\n",
    "X_test_dask = da.from_array(X_test.values, chunks=(len(X_test)//2, X_test.shape[1])) # convert X testing set to dask array with 2 partitions\n",
    "\n",
    "print(f\"Training data partitions: {X_train_dask.npartitions}\") # output number of partitions in the dask array\n",
    "print(f\"Training data shape: {X_train_dask.shape}\") # output shape of the dask array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0396623c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM with distributed learning\n",
    "lgb_dist_clf = lgb.LGBMClassifier(\n",
    "    random_state=65,\n",
    "    n_jobs=-1  # utilize all available CPU cores\n",
    ")\n",
    "\n",
    "lgb_dist_clf.fit(X_train, y_train) # fit on the heart disease training data\n",
    "\n",
    "y_pred_lgb_dist = lgb_dist_clf.predict(X_test) # make predictions using distrubted LightGBM model\n",
    "\n",
    "print(\"LightGBM Distributed Learning Results:\") # header for results\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred_lgb_dist))  # output confusion matrix of the distributed lightgbm model\n",
    "\n",
    "print(classification_report(y_test, y_pred_lgb_dist)) # output classification report of the distributed lightgbm model\n",
    "\n",
    "print('ROC AUC:', roc_auc_score(y_test, y_pred_lgb_dist)) # output ROC AUC score of the distributed lightgbm model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da69e340",
   "metadata": {},
   "source": [
    "### **Finding Best Hyperparameters for the Distributed LightGBM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ea8dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_dist_grid = DaskGridSearchCV(\n",
    "    lgb.LGBMClassifier(random_state=65, n_jobs=1), # instance the lgbm classifier again\n",
    "    lgb_param_grid, # use the same custom lgb hyperparameter grid\n",
    "    cv=10, # use 10 cross folds for cross validation\n",
    "    scoring='roc_auc' # score based on ROC AUC\n",
    ") # setup dask grid search cv for distributed learning to find best hyperparameters for lightgbm model\n",
    "\n",
    "lgb_dist_grid.fit(X_train, y_train) # perform the grid search on the training data\n",
    "\n",
    "print('Best LightGBM Distributed Params:', lgb_dist_grid.best_params_) # output the best hyperparameters found for distributed lightgbm model\n",
    "\n",
    "print('Best LightGBM Distributed CV ROC AUC:', lgb_dist_grid.best_score_) # output the best cross-validated ROC AUC score from the distributed grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd80559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "client.close() # close the dask client as we have found best hyperparameters and distributed learning is done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068e3e1d",
   "metadata": {},
   "source": [
    "## **Evaluation of Gradient Boosting Model Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be859f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# XGBoost evaluation\n",
    "results = {}\n",
    "\n",
    "precision_xgb = precision_score(y_test, y_pred_xgb)\n",
    "recall_xgb = recall_score(y_test, y_pred_xgb)\n",
    "accuracy_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "f1_xgb = f1_score(y_test, y_pred_xgb)\n",
    "roc_auc_xgb = roc_auc_score(y_test, y_pred_xgb)\n",
    "\n",
    "results['XGBoost'] = {\n",
    "    'Precision': precision_xgb,\n",
    "    'Recall': recall_xgb,\n",
    "    'Accuracy': accuracy_xgb,\n",
    "    'F1-Score': f1_xgb,\n",
    "    'ROC AUC': roc_auc_xgb,\n",
    "    'Inference Time (s)': inference_time_xgb,\n",
    "}\n",
    "\n",
    "print(\"XGBoost Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Precision:  {precision_xgb:.4f}\")\n",
    "print(f\"Recall:     {recall_xgb:.4f}\")\n",
    "print(f\"Accuracy:   {accuracy_xgb:.4f}\")\n",
    "print(f\"F1-Score:   {f1_xgb:.4f}\")\n",
    "print(f\"ROC AUC:    {roc_auc_xgb:.4f}\")\n",
    "print(f\"Inference Time: {inference_time_xgb:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5cf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM (non-distributed) evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LightGBM (Non-Distributed) Model Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_lgb = lgb_clf.predict(X_test)\n",
    "inference_time_lgb = time.time() - start_time\n",
    "\n",
    "precision_lgb = precision_score(y_test, y_pred_lgb)\n",
    "recall_lgb = recall_score(y_test, y_pred_lgb)\n",
    "accuracy_lgb = accuracy_score(y_test, y_pred_lgb)\n",
    "f1_lgb = f1_score(y_test, y_pred_lgb)\n",
    "roc_auc_lgb = roc_auc_score(y_test, y_pred_lgb)\n",
    "\n",
    "results['LightGBM'] = {\n",
    "    'Precision': precision_lgb,\n",
    "    'Recall': recall_lgb,\n",
    "    'Accuracy': accuracy_lgb,\n",
    "    'F1-Score': f1_lgb,\n",
    "    'ROC AUC': roc_auc_lgb,\n",
    "    'Inference Time (s)': inference_time_lgb,\n",
    "}\n",
    "\n",
    "print(f\"Precision:  {precision_lgb:.4f}\")\n",
    "print(f\"Recall:     {recall_lgb:.4f}\")\n",
    "print(f\"Accuracy:   {accuracy_lgb:.4f}\")\n",
    "print(f\"F1-Score:   {f1_lgb:.4f}\")\n",
    "print(f\"ROC AUC:    {roc_auc_lgb:.4f}\")\n",
    "print(f\"Inference Time: {inference_time_lgb:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b513a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LightGBM (distributed) evaluation\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"LightGBM (Distributed) Model Evaluation\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_time = time.time()\n",
    "y_pred_lgb_dist = lgb_dist_clf.predict(X_test)\n",
    "inference_time_lgb_dist = time.time() - start_time\n",
    "\n",
    "precision_lgb_dist = precision_score(y_test, y_pred_lgb_dist)\n",
    "recall_lgb_dist = recall_score(y_test, y_pred_lgb_dist)\n",
    "accuracy_lgb_dist = accuracy_score(y_test, y_pred_lgb_dist)\n",
    "f1_lgb_dist = f1_score(y_test, y_pred_lgb_dist)\n",
    "roc_auc_lgb_dist = roc_auc_score(y_test, y_pred_lgb_dist)\n",
    "\n",
    "results['LightGBM Distributed'] = {\n",
    "    'Precision': precision_lgb_dist,\n",
    "    'Recall': recall_lgb_dist,\n",
    "    'Accuracy': accuracy_lgb_dist,\n",
    "    'F1-Score': f1_lgb_dist,\n",
    "    'ROC AUC': roc_auc_lgb_dist,\n",
    "    'Inference Time (s)': inference_time_lgb_dist,\n",
    "}\n",
    "\n",
    "print(f\"Precision:  {precision_lgb_dist:.4f}\")\n",
    "print(f\"Recall:     {recall_lgb_dist:.4f}\")\n",
    "print(f\"Accuracy:   {accuracy_lgb_dist:.4f}\")\n",
    "print(f\"F1-Score:   {f1_lgb_dist:.4f}\")\n",
    "print(f\"ROC AUC:    {roc_auc_lgb_dist:.4f}\")\n",
    "print(f\"Inference Time: {inference_time_lgb_dist:.4f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a0faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Results table\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Comprehensive Model Comparison Table\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "metrics = ['Precision', 'Recall', 'Accuracy', 'F1-Score', 'ROC AUC']\n",
    "\n",
    "print(results_df.to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "036e39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics bar chart + inference time\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "fig.suptitle('Model Performance Metrics Comparison', fontsize=16, fontweight='bold')\n",
    "\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, metric in enumerate(metrics):\n",
    "    ax = axes[idx // 3, idx % 3]\n",
    "    values = [results[model][metric] for model in results.keys()]\n",
    "    bars = ax.bar(results.keys(), values, color=colors)\n",
    "\n",
    "    ax.set_ylabel(metric, fontweight='bold')\n",
    "    ax.set_ylim([0, 1.0])\n",
    "    ax.set_title(metric)\n",
    "\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.3f}',\n",
    "        ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "ax = axes[1, 2]\n",
    "inference_times = [results[model]['Inference Time (s)'] for model in results.keys()]\n",
    "bars = ax.bar(results.keys(), inference_times, color=colors)\n",
    "\n",
    "ax.set_ylabel('Time (seconds)', fontweight='bold')\n",
    "ax.set_title('Inference Time')\n",
    "\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2.0, height, f'{height:.4f}s',\n",
    "    ha='center', va='bottom', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4730b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of all metrics\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "sns.heatmap(results_df, annot=True, fmt='.4f', cmap='RdYlGn', cbar_kws={'label': 'Score'}, ax=ax)\n",
    "\n",
    "ax.set_title('Model Performance Heatmap', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61c65af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Radar chart comparison\n",
    "categories = ['Precision', 'Recall', 'Accuracy', 'F1-Score', 'ROC AUC']\n",
    "angles = [n / float(len(categories)) * 2 * pi for n in range(len(categories))]\n",
    "angles += angles[:1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10), subplot_kw=dict(projection='polar'))\n",
    "\n",
    "colors_radar = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, model in enumerate(results.keys()):\n",
    "    values = [results[model][cat] for cat in categories]\n",
    "    values += values[:1]\n",
    "    \n",
    "    ax.plot(angles, values, 'o-', linewidth=2, label=model, color=colors_radar[idx])\n",
    "    ax.fill(angles, values, alpha=0.15, color=colors_radar[idx])\n",
    "\n",
    "ax.set_xticks(angles[:-1])\n",
    "ax.set_xticklabels(categories)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_title('Model Performance Radar Chart', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.legend(loc='upper right', bbox_to_anchor=(1.3, 1.1))\n",
    "ax.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f39be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Individual metrics line plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "x_pos = np.arange(len(metrics))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, model in enumerate(results.keys()):\n",
    "    values = [results[model][metric] for metric in metrics]\n",
    "    \n",
    "    ax.plot(x_pos, values, marker='o', label=model, linewidth=2, markersize=8, color=colors[idx])\n",
    "\n",
    "ax.set_xlabel('Metrics', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Performance Trend Across Metrics', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos)\n",
    "ax.set_xticklabels(metrics, rotation=15)\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670553a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grouped bar chart for key metrics\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "key_metrics = ['Precision', 'Recall', 'F1-Score']\n",
    "x_pos = np.arange(len(key_metrics))\n",
    "width = 0.25\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "for idx, model in enumerate(results.keys()):\n",
    "    values = [results[model][metric] for metric in key_metrics]\n",
    "    \n",
    "    ax.bar(x_pos + idx * width, values, width, label=model, color=colors[idx])\n",
    "\n",
    "ax.set_xlabel('Metrics', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Score', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Key Classification Metrics Comparison', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x_pos + width)\n",
    "ax.set_xticklabels(key_metrics)\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.legend(fontsize=10)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654c074c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrices\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
    "cm_lgb = confusion_matrix(y_test, y_pred_lgb)\n",
    "cm_lgb_dist = confusion_matrix(y_test, y_pred_lgb_dist)\n",
    "\n",
    "cms = [cm_xgb, cm_lgb, cm_lgb_dist]\n",
    "titles = ['XGBoost', 'LightGBM', 'LightGBM Distributed']\n",
    "\n",
    "for idx, (cm, title) in enumerate(zip(cms, titles)):\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx], cbar=False)\n",
    "    \n",
    "    axes[idx].set_title(f'{title} Confusion Matrix', fontweight='bold')\n",
    "    axes[idx].set_ylabel('True Label')\n",
    "    axes[idx].set_xlabel('Predicted Label')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a81b3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model efficiency (accuracy vs inference time)\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1']\n",
    "\n",
    "model_names = list(results.keys())\n",
    "accuracies = [results[model]['Accuracy'] for model in model_names]\n",
    "times = [results[model]['Inference Time (s)'] for model in model_names]\n",
    "\n",
    "scatter = ax.scatter(times, accuracies, s=300, c=colors, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "for i, model in enumerate(model_names):\n",
    "    ax.annotate(\n",
    "        model,\n",
    "        (times[i], accuracies[i]),\n",
    "        xytext=(10, 10),\n",
    "        textcoords='offset points',\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        bbox=dict(boxstyle='round,pad=0.5', facecolor=colors[i], alpha=0.3),\n",
    "    )\n",
    "\n",
    "ax.set_xlabel('Inference Time (seconds)', fontweight='bold', fontsize=12)\n",
    "ax.set_ylabel('Accuracy', fontweight='bold', fontsize=12)\n",
    "ax.set_title('Model Efficiency: Accuracy vs Inference Time', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.set_ylim([min(accuracies) - 0.05, 1.05])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06d3827",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking analysis\n",
    "print(\"\" + \"=\" * 80)\n",
    "print(\"MODEL RANKING ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for metric in metrics + ['Inference Time (s)']:\n",
    "    print(f\"{metric} Rankings:\")\n",
    "\n",
    "    metric_values = [(model, results[model][metric]) for model in results.keys()]\n",
    "    \n",
    "    if metric == 'Inference Time (s)':\n",
    "        metric_values.sort(key=lambda x: x[1])\n",
    "        print(\"(Lower is Better)\")\n",
    "\n",
    "    else:\n",
    "        metric_values.sort(key=lambda x: x[1], reverse=True)\n",
    "        print(\"(Higher is Better)\")\n",
    "\n",
    "    for rank, (model, value) in enumerate(metric_values, 1):\n",
    "        print(f\"  {rank}. {model}: {value:.4f}\")\n",
    "              \n",
    "print(\"\" + \"=\" * 80)\n",
    "print(\"OVERALL MODEL RANKING (Average Performance Across All Metrics)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "overall_ranks = {model: [] for model in results.keys()}\n",
    "\n",
    "for metric in metrics + ['Inference Time (s)']:\n",
    "    metric_values = [(model, results[model][metric]) for model in results.keys()]\n",
    "\n",
    "    if metric == 'Inference Time (s)':\n",
    "        metric_values.sort(key=lambda x: x[1])\n",
    "\n",
    "    else:\n",
    "        metric_values.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    for rank, (model, _) in enumerate(metric_values, 1):\n",
    "        overall_ranks[model].append(rank)\n",
    "                                    \n",
    "avg_ranks = {model: np.mean(ranks) for model, ranks in overall_ranks.items()}\n",
    "sorted_models = sorted(avg_ranks.items(), key=lambda x: x[1])\n",
    "                       \n",
    "for rank, (model, avg_rank) in enumerate(sorted_models, 1):\n",
    "    print(f\"{rank}. {model}: Average Rank = {avg_rank:.2f}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Evaluation Complete\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706ffb45",
   "metadata": {},
   "source": [
    "### **SHAP Explainations of both Gradient Boosting Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3478dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation for XGBoost model\n",
    "explainer_xgb = shap.TreeExplainer(xgb_clf) # setup SHAP tree explainer for the xgboost model\n",
    "shap_values_xgb = explainer_xgb.shap_values(X_test.values) # compute SHAP values for the xgboost model using the testing data of the heart disease dataset\n",
    "\n",
    "shap.summary_plot(shap_values_xgb, X_test, show=False) # summary plot the calculated SHAP values for the xgboost model\n",
    "\n",
    "plt.title('SHAP Summary Plot for XGBoost')\n",
    "\n",
    "plt.show() # output SHAP summary plot for the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fba2adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation for non-distributed LightGBM\n",
    "explainer_lgb = shap.TreeExplainer(lgb_clf) # setup SHAP tree explainer for the non-distributed lightgbm model\n",
    "shap_values_lgb = explainer_lgb.shap_values(X_test.values) # compute SHAP values for the non-distributed lightgbm model using the testing data of the heart disease dataset\n",
    "\n",
    "shap.summary_plot(shap_values_lgb, X_test, show=False) # summary plot the calculated SHAP values for the non-distributed lightgbm model\n",
    "\n",
    "plt.title('SHAP Summary Plot for LightGBM')\n",
    "\n",
    "plt.show() # output SHAP summary plot for the non-distributed lightgbm model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba599bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP explanation for LightGBM with Dask Distributed Learning\n",
    "explainer_lgb_dist = shap.TreeExplainer(lgb_dist_clf) # setup SHAP tree explainer for the lightgbm model trained with dask distributed learning\n",
    "shap_values_lgb_dist = explainer_lgb_dist.shap_values(X_test.values) # compute SHAP values for the lightgbm model trained with dask distributed learning using the testing data of the heart disease dataset\n",
    "\n",
    "shap.summary_plot(shap_values_lgb_dist, X_test, show=False) # plot the calculated SHAP values for the lightgbm model trained with dask distributed learning\n",
    "\n",
    "plt.title('SHAP Summary Plot for LightGBM with Dask')\n",
    "\n",
    "plt.show() # output SHAP summary plot for the lightgbm model trained with dask distributed learning"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
